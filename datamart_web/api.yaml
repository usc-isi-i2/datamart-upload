openapi: 3.0.2
info:
  title: ISI Datamart Link Panel
  version: 0.1.0
  contact:
    name: API Support
    email: kyao@isi.edu
  description: |
    ## This is the ISI Datamart Server

    ##  Most frequently used endpoints are:
      * **/search** Find datasets for augmentation, given a supplied dataset.
      * **/search_without_data** Find datasets without a supplied dataset.
      * **/augment** Augment a supplied dataset (either training dataset or testing dataset) using a result returned by **/search**
      * **/upload** Upload a dataset to the datamart. The dataset can be a file or an URL.

    Instructions on how use these endpoints are documented below.

#servers:
#  - url: http://dsbox02.isi.edu:9000
#    description: Main (production) server
  # - url: http://staging-api.example.com
  #   description: Optional server description, e.g. Internal staging server for testing
paths:
  /wikifier:
    post:
      tags:
        - Search API
      summary: Do wikification before search
      description: |

        Performs wikification, i.e. add additional columns to the supplied dataset using Wikidata for entitiy
        resolution. Column values added are Wikidata Qnodes representing entities.

        This method automatically tries to find the columns in the supplied dataset to use for wikification. Also, the
        user can specify which columns to use and the specific wikifier. For numeric columns use the **indentifier**
        wikifier. For text column use the **new_wikifier**.

        Required parameter:
          * **data**: The supplied data (a path to the csv file or a d3m dataset id).

        Optional parameters:
          * **format**: Return format (csv format or d3m format with metadata).
          * **threshold**: minimum coverage ratio for a wikidata columns to be appended.
      # * **column**: specifies which column from the complementary datamart dataset to append to the supplied data. If not given, all columns are append.
      requestBody:
        content:
          application/json:
            schema:
              type: array
              items:
                type: object
                properties:
                  column:
                    type: integer
                  wikifier_choice:
                    type: string
            examples:
              automatic_column_selection:
                value: null
              example_for_wikification_column_selection:
                value:
                  - column: 1
                    wikifier_choice: identifier
                  - column: 2
                    wikifier_choice: new_wikifier
                  - column: 3
                    wikifier_choice: new_wikifier
                  - column: 5
                    wikifier_choice: identifier
                description: 'Column is an integer specifying the column to use for wikification.<br> Wikifier_choice is an optional parameter, it includes three choices: **identifier**, **new_wikifier** and **automatic**. For a numeric column, use **identifier**. For a text column, use **new_wikifier**. Use **automatic** to let the datamrt decide.'
      parameters:
        - in: query
          name: data
          description: '<p>Supplied data file. It could be a filepath or a d3m dataset id (for example: "DA_poverty_estimation"). </p>'
          schema:
            type: string
          required: true
          examples:
            filepath:
              value: https://raw.githubusercontent.com/usc-isi-i2/datamart-upload/rest_api_stable/datamart_web/test_search_data.csv
            d3m-dataset-id:
              value: DA_poverty_estimation
        - in: query
          name: format
          description: Optional parameter, download format(general csv format or d3m format), default is csv.
          required: false
          schema:
            type: string
            enum: [csv, d3m]
        - in: query
          name: threshold
          description: Optional parameter, minimum coverage ratio for a wikidata columns to be appended. The range is (0,1) and default is 0.7.
          required: false
          schema:
            type: number
          example: 0.7
      responses:
        '200':
          description: A zip file
          content:
            application/zip:
              schema:
                 type: string
                 format: binary
        '400':
          description: "Error"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /search:
    post:
      tags:
        - Search API
      summary: Search
      description: |
        The main method for finding datasets for augmentation. Typically, this method is the first method to call.

        The only required parameter is the supplied_data parameter that provides the dataset. The JSON query is not required.

        Required parameter:
          * **data**: Dataset for search. The dataset can be specified as a path to a csv file, or a d3m dataset id (for example: "DA_poverty_estimation"). Also, the datset can be uploaded from the request body as a string in the format of csv/zip/pkl, e.g. {"data": "...dataset..."}. The dataset sent from body takes precedent.

        Optional parameters:
          * **max_return_docs**: Number of search results to return. If not given, the default value is 20.
          * **run_wikifier** Boolean value, if true perform object resolution using ISI wikifier to find additional datasets.

        Request body:
          * **query_json**: A JSON query to specify additional search constraints You can follow the example <a href="https://raw.githubusercontent.com/usc-isi-i2/datamart-userend/d3m/examples/sample_time_search_variables.json">here</a>. You can also check the examples on */search_without_data* api section.
          * **data**: A supplied data for searching, it could be in pkl/zip/csv format.
      parameters:
        - in: query
          name: max_return_docs
          description: Max return docs (default value as 20)
          schema:
            type: integer
          example: 20
        - in: query
          name: data
          description: '<p>Supplied data file. Could be a d3m-dataset-id or a filepath or leave blank and send from body.</p>'
          schema:
            type: string
          examples:
            filepath:
              value: https://raw.githubusercontent.com/usc-isi-i2/datamart-upload/rest_api_stable/datamart_web/test_search_data.csv
            d3m-dataset-id:
              value: DA_poverty_estimation
        - in: query
          name: run_wikifier
          description: whether to run wikifier on supplied data or not. Default value is true. If not run wikifier, the search speed will be quicker but the wikidata related parts will be missed.
          schema:
            type: string
            default: "true"
            enum: ["true", "false"]
        - in: query
          name: consider_wikifier_columns_only
          description: if set to true, only search results with wikifiered left columns(from supplied data) will be considered and returned.
          schema:
            type: string
            default: "false"
            enum: ["true", "false"]
        - in: query
          name: augment_with_time
          description: if set to true, the time column(if exists), will be also considered (totally 2 columns as left columns for join) during generating search results.
          schema:
            type: string
            default: "false"
            enum: ["true", "false"]
        - in: query
          name: consider_time
          description: if set to true, the time column(if exists), will be also considered (only time columns for join, different from augment_with_time) during generating search results. If set to false, no time columns will be considered.
          schema:
            type: string
            default: "true"
            enum: ["true", "false"]
        # - in: formData
          # name: query_json
          
      requestBody:
        content:
          multipart/form-data:
            schema:
              type: object
              properties:
                data:
                  type: string
                  format: binary
                  description: 'supplied data file in csv for general format or zip/pkl for d3m format'
                query_json:
                  type: string
                  format: json
                  description: "keywords and variables for search"
                  schema:
                      $ref: "#/components/schemas/Query"
                  examples:
                      keyword_search:
                        summary: "Search using the keywords 'poverty rate' and 'education'"
                        value:
                          keywords:
                            - "poverty rate"
                            - "education"
                          variables: []
      responses:
        '200':
          description: successful operation
          content:
            application/json:
              schema:
                type: array
                items:
                  $ref: "#/components/schemas/Results"
        '400':
          description: "Error"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /search_without_data:
    post:
      tags:
        - Search API
      summary: Search by keywords or variables
      description: |
        Find datasets by keywords and/or variables.

        **Attention:** The search results found here cannot be used for augmentation. If you want to use the search
          result for augmentation, please use **/search** endpoint instead!

        Required request body:
          * **query_json**: A JSON query to specify additional search constraints You can follow the example <a href="https://raw.githubusercontent.com/usc-isi-i2/datamart-userend/d3m/examples/sample_time_search_variables.json">here</a>. <br> Please see detail explainations on example part.
      requestBody:
        content:
          application/json:
            description: "keywords and variables for search"
            schema:
              $ref: '#/components/schemas/Query'
            examples:
              temporal_variable:
                description: "Search datasets contain temporal information between `2018-01-01` and `2018-07-01`.<br> DateTime for `start` and `end` format could be: '2019-01' or '2019-01-01' or '2019-01-01T10:10:10'. Granularity value could be **year**, **month**, **day**, **hour**, **minute** ,or **second**. <br> Datamart will only return the searched results with time column and granularity larger than given value. <br> For example, if a dataset has a time coulmn in `year` level but given constrains with `month`, this result would not be displayed. "
                value:
                  keywords:
                    - "new york"
                    - "taxi"
                  variables:
                    temporal_variable:
                      start: 2018-01-01T01
                      end: 2018-07-01T01
                      granularity: hour
              keywords_searching:
                description: "Search using the keywords `poverty rate` and `education`, no variable constrains."
                value:
                  keywords:
                    - "poverty rate"
                    - "education"
                  variables: {}
      responses:
        '200':
          description: successful operation
          content:
            application/json:
              schema:
                type: array
                items:
                  $ref: '#/components/schemas/Results'
        '400':
          description: "Error"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /augment:
    post:
      tags:
        - Augment API
      summary: Augment dataset
      description: '<p>
        This method augments the given dataset using a search result returned by the **/search** endpoint. The given
        dataset has to be structurally the same as the dataset used during search. The content of the dataset can
        differ. During search the training dataset is given to the **/serach** endpoint. Then, the search result can be
        used either to augment the training dataset or the testing dataset.

        This method only returns the rows of the dataset that joins well with the complementary datamart dataset. If no rows can be joined, an error will be raised.

        **Notice**: To keep the d3m system still runable, the augment opeartion will not add extra rows. If same join key found on the dataset specified on complementary dataset, only the first row will be used to augment.
        Request body:
          * **task**: A string containing a result return by the **/search** endpoint.

        Required parameters are:
          * **data**: The supplied data (a path to the csv file or a d3m dataset id).
          * **format**: Return format (csv format or d3m format with metadata).

        Optional parameters:
          * **destination**: If given, the system saves the augmented dataset to the given path, and returns the
              path. If not given, it returns the results to user as download function.
          * **columns**: specifies which columns from the complementary datamart dataset to append to the supplied data. If not given, all columns are append.
          * **run_wikifier**: If true, run the wikifier to add additional columns based on entity resolution.
          * **use_cache**: If true, the system will use cached result if exists. If choose **default**, the system will determine base on the system config.
          </p>'

      requestBody:
        content:
          multipart/form-data:
            schema:
              type: object
              properties:
                data:
                  type: string
                  format: binary
                  description: 'supplied data file in csv for general format or zip/pkl for d3m format'
                task:
                  type: string
                  description: 'The search result task (json-like str) from Search API you can try to copy the content from <a href="https://raw.githubusercontent.com/usc-isi-i2/datamart-upload/rest_api_stable/datamart_web/sample_search_result_wikidata.txt">example1</a> or <a href="https://raw.githubusercontent.com/usc-isi-i2/datamart-upload/rest_api_stable/datamart_web/sample_search_result_general.txt">example2</a>'
              required:
                - task
      parameters:
        - in: query
          name: data
          description: '<p>Supplied data file. It could be a d3m-dataset-id or a filepath</p>'
          schema:
            type: string
          required: false
          examples:
            filepath:
              value: https://raw.githubusercontent.com/usc-isi-i2/datamart-upload/rest_api_stable/datamart_web/test_search_data.csv
            d3m-dataset-id:
              value: DA_poverty_estimation
        - in: query
          name: format
          description: Download format(general csv format or d3m format)
          required: true
          schema:
            type: string
            enum: [csv, d3m]
        - in: query
          name: columns
          schema:
            type: array
            items:
              type: integer
          style: form
          explode: false
          example: [1, 3]
          description: Optional parameter, a str indicate the list of column indices from the Datamart dataset that should be appended. See format example below, If not given, all possible columns from search results will be appended. In each item, input one integer.
        - in: query
          name: destination
          schema:
            type: string
          description: If given, the augmented results(in csv or d3m) will be saved to the given path. If not given, the augmented result will be returned to the user for download. See the example for the destination format
          example: /Users/claire/Desktop
        - in: query
          name: run_wikifier
          description: "whether to run wikifier on supplied data and search result(only applied for genearl search results) or not. Default value is true. The download will failed if the joining hint columns are adapted from wikifier's columns."
          schema:
            type: string
            enum: [true, false]
        - in: query
          name: use_cache
          description: "whether to use cached results of the system or not. If cache exists, the processing speed will be quicker. If choose default, the system will use the default config option(currently it is **true**)."
          schema:
            type: string
            enum: [default, true, false]
      responses:
        '200':
          description: A zip file
          content:
            application/zip:
              schema:
                 type: string
                 format: binary
        '400':
          description: "Error"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /download:
    post:
      tags:
        - Download API
      summary: Download a dataset
      description: |
        The method downloads the datamart dataset specified by a search result returned by the Search API method.

        This method only returns rows of the dataset that matches well with the complementary datamart dataset. If no rows can be joined, an error is raised.

        Required parameters:
          * **data**: The supplied data (a path to the csv file or a d3m dataset id).
          * **format**: Return format (csv format or d3m format with metadata).

        Optional parameters:
          * **run_wikifier**: If true, run wikifier to add additional Qnode columns based on entity resolution. Default is true.

        Request body:
          * **task**: An item in string format from the search result list return by the Search API (Required).
      requestBody:
        content:
          multipart/form-data:
            schema:
              type: object
              properties:
                task:
                  type: string
                  description: 'The search result task (json-like str) from Search API you can try to copy the content from <a href="https://raw.githubusercontent.com/usc-isi-i2/datamart-upload/rest_api_stable/datamart_web/sample_search_result_wikidata.txt">example1</a> or <a href="https://raw.githubusercontent.com/usc-isi-i2/datamart-upload/rest_api_stable/datamart_web/sample_search_result_general.txt">example2</a>'
              required:
                - task
      parameters:
        - in: query
          name: data
          description: '<p>Supplied data file. It could be a d3m-dataset-id or a filepath</p>'
          schema:
            type: string
          required: true
          examples:
            filepath:
              value: https://raw.githubusercontent.com/usc-isi-i2/datamart-upload/rest_api_stable/datamart_web/test_search_data.csv
            d3m-dataset-id:
              value: DA_poverty_estimation
        - in: query
          name: format
          description: Download format(general csv format or d3m format)
          required: true
          schema:
            type: string
            enum: [csv, d3m]
        - in: query
          name: run_wikifier
          description: "whether to run wikifier on supplied data and search result(only applied for genearl search results) or not. Default value is true. The download will failed if the joining hint columns are adapted from wikifier's columns."
          schema:
            type: string
            enum: [true, false]
      responses:
        '200':
          description: A zip file
          content:
            application/zip:
              schema:
                 type: string
                 format: binary
        '400':
          description: "Error"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /download_metadata/{id}:
    get:
      tags:
        - Download API
      summary: Download the dataset metadata with given id
      description: 'The method downloads the datamart dataset metadata associated with the datamart id. Different from the /download/{id} method, this method returns the dataset metadata. <br>If a wikidata format id is given, it will return maximum 100 random Q nodes with those properties metadata. <br><br>One parameter is required: <br> **id**: The datamart id, can be found from search result.'
      parameters:
        - in: path
          name: id
          description: ID of datamart
          required: true
          schema:
            type: string
          examples:
            example 1:
              value: D20eb2d7f-aafb-443e-8bf2-132ef103e98b
            example 2:
              value: wikidata_search_on___P1082___P2046___P571___with_column_FIPS_wikidata
      responses:
        '200':
          description: array
          content:
            application/json:
              schema:
                 type: array
                 items:
                  $ref: '#/components/schemas/Metadata'
        '400':
          description: "Error"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /download/{id}:
    get:
      tags:
        - Download API
      summary: Download the dataset with given id
      description: 'The method downloads the datamart dataset associated with the datamart id. Different from the /download method, this method returns the whole dataset without join hint columns. <br> If a wikidata format id is given, it will return maximum 100 random Q nodes with those properties. <br> <br>Two parameters are required: <br> **id**: The datamart id, can be found from search result. <br> **format**: Return format (csv format or d3m format with metadata).'
      parameters:
        - in: path
          name: id
          description: ID of datamart
          required: true
          schema:
            type: string
          examples:
            example 1:
              value: D20eb2d7f-aafb-443e-8bf2-132ef103e98b
            example 2:
              value: wikidata_search_on___P1082___P2046___P571___with_column_FIPS_wikidata
        - in: query
          name: format
          description: Download format(general csv format or d3m format)
          required: true
          schema:
            type: string
            enum: [csv, d3m]
      responses:
        '200':
          description: A zip file
          content:
            application/zip:
              schema:
                 type: string
                 format: base64
        '400':
          description: "Error"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /upload:
    post:
      tags:
        - Upload API
      summary: Upload dataset with metadata
      description: |
        This is the RECOMMENDED method to upload datasets. This an all-in-one method for uploading one or more datasets
        to the datamart. The dataset can be a regular file, or it can be referenced by an URL. An URL may reference more
        than one dataset.

        The method automatically generates metadata for the datasets.

        Required parameters:
          * **file_type**: A string indicating the type of the data source, currently only **online_csv** is suported.
          * **username**: The username of the upload user. If you do not have an account, ask the datamart
              administrator. If you just want to try out the upload functionality, use the **/upload/test** endpoint.
          * **password**: The password of the upload user.

        Optional parameters:
          * **url**: The address to the dataset resource. An alternative way is upload a local file using request body.
          * **run_wikifier**: If true, runs the wikifier to add additional Wikidata Qnode columns.
          * **title**: The name of the dataset.
          * **description**: Text description of the dataset.
          * **keywords**: Keywords associated with the dataset.

        Request body:
          * Filepath to dataset.
      requestBody:
        content:
          multipart/form-data:
            schema:
              type: object
              properties:
                upload_file:
                  type: string
                  format: binary
                  description: 'Dataset file (.csv or .csv.gz) need to upload to datamart'
      parameters:
        - in: query
          name: url
          schema:
            type: string
          example: https://raw.githubusercontent.com/usc-isi-i2/datamart-upload/rest_api_stable/datamart_web/test_upload_data.csv
          # required: true
          description: Url of input file
        # - in: formData
        #   name: upload_file
        #   schema:
        #     type: file
        #   description: The file of the dataset to upload to datamart
        - in: query
          name: upload_async
          description: The upload method, if set to be true, the upload process will be assigned to background process and no need to waiting on this page. This service also request server's Redis working. Currently it will only work on dsbox02 side.
          schema:
            type: string
            enum: [true, false]
          example: true
        - in: query
          name: run_wikifier
          description: 'Whether to run wikifier on the uploaded dataset or not. <br> Default value is auto. Wikifier step will skip if the size(row number * column number) of the dataset is larger than **100000**'
          schema:
            type: string
            example: auto
            enum: [auto, true, false]
        - name: need_process_columns
          in: query
          schema:
            type: string
          required: false
          description: 'User can speicify which columns in numbers want to be indexed and saved to datamart. If given, only the speicified columns will be indexed and can be considered as join columns. e.g.: **2, 3, 5** this means only index column No.2, 3 and 5. If not given, all columns will be parsed. <br> If has multiple dataset, split with **||**. For example, **2,3 || 4, 5 || None **'
        - name: file_type
          in: query
          schema:
            type: string
            enum:
              - online_csv
          required: true
          description: Online file type, currently only support online_csv
        - name: title
          in: query
          schema:
            type: string
          required: false
          description: 'Name of the dataset, if the url contains multiple dataset, please provide each title, split with mark "||". <br> For example, title1||title2'
        - name: description
          in: query
          schema:
            type: string
          required: false
          description: 'Description of the dataset, if the url contains multiple dataset, please provide each description, split with mark "||". <br> For example, This is description for first dataset||This is description for second dataset'
        - name: keywords
          in: query
          schema:
            type: string
          required: false
          description: 'Keywords or tags to describe the dataset content, if the url contains multiple dataset, please provide each keywords, split with mark "||". <br> For example, keywords1_1, keywords1_2||keywords2_1, keywords2_2'
        - name: username
          in: query
          schema:
            type: string
          required: true
          description: the username for the upload user
        - name: password
          in: query
          schema:
            type: string
            format: password
          required: true
          description: the password for the upload user
      responses:
        '200':
          description: successful operation
          content:
            application/text:
              schema:
                type: string
        '400':
          description: "Error"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /upload/test:
    post:
      tags:
        - Upload API
      summary: Upload dataset with metadata
      description: 'Same function as upload. The only difference is it will upload the dataset to a test database which will not influence the main one'
      requestBody:
        content:
          multipart/form-data:
            schema:
              type: object
              properties:
                upload_file:
                  type: string
                  format: binary
                  description: 'Dataset file (.csv or .csv.gz) need to upload to datamart'
      parameters:
        - in: query
          name: url
          schema:
            type: string
          example: https://raw.githubusercontent.com/usc-isi-i2/datamart-upload/rest_api_stable/datamart_web/test_upload_data.csv
          # required: true
          description: Url of input file
        # - in: formData
        #   name: upload_file
        #   schema:
        #     type: file
        #   description: The file of the dataset to upload to datamart
        - in: query
          name: run_wikifier
          description: 'whether to run wikifier on the uploaded dataset or not. <br> Default value is auto. Wikifier step will skip if the size(row number * column number) of the dataset is larger than **100000**'
          schema:
            type: string
            example: auto
            enum: [auto, true, false]
        - name: need_process_columns
          in: query
          schema:
            type: string
          required: false
          description: 'User can speicify which columns in numbers want to be indexed and saved to datamart. If given, only the speicified columns will be indexed and can be considered as join columns. e.g.: **2, 3, 5** this means only index column No.2, 3 and 5. If not given, all columns will be parsed. <br> If has multiple dataset, split with **||**. For example, **2,3 || none || 4, 5 ** which means no limit on second dataset'
        - name: file_type
          in: query
          schema:
            type: string
            enum:
              - online_csv
          required: true
          description: Online file type, currently only support online_csv
        - name: title
          in: query
          schema:
            type: string
          required: false
          description: 'Name of the dataset, if the url contains multiple dataset, please provide each title, split with mark "||". <br> For example, title1||title2'
        - name: description
          in: query
          schema:
            type: string
          required: false
          description: 'Description of the dataset, if the url contains multiple dataset, please provide each description, split with mark "||". <br> For example, This is description for first dataset||This is description for second dataset'
        - name: keywords
          in: query
          schema:
            type: string
          required: false
          description: 'Keywords or tags to describe the dataset content, if the url contains multiple dataset, please provide each keywords, split with mark "||". <br> For example, keywords1_1, keywords1_2||keywords2_1, keywords2_2'
        - name: username
          in: query
          schema:
            type: string
          required: true
          description: the username for the upload user
        - name: password
          in: query
          schema:
            type: string
            format: password
          required: true
          description: the password for the upload user
      responses:
        '200':
          description: successful operation
          content:
            application/text:
              schema:
                type: string
        '400':
          description: "Error"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /upload/check_upload_status:
    post:
      tags:
        - Upload API
      summary: an api used to check the uploading status
      description: 'This function is used for user to check the processes of uploading'
      parameters:
        - in: query
          name: job_ids
          schema:
            type: string
          example: 'c6b7b967-aa75-47e4-b206-ec3447fbc90c'
          required: false
          description: the job ids of the uploading processes
      responses:
        '200':
          description: successful operation
          content:
            application/text:
              schema:
                type: string
        '400':
          description: "Error"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /upload/add_upload_user:
    post:
      tags:
        - Upload API
      summary: an api used to add upload user
      description: 'This function is used for adding user for uploading datasets'
      parameters:
        - name: token
          in: query
          schema:
            type: string
          required: true
          description: token for access to create user account
          example: ''
        - name: username
          in: query
          schema:
            type: string
          required: true
          description: username of the account
          example: ''
        - name: password
          in: query
          schema:
            type: string
          required: true
          description: password of the account
          example: ''
      responses:
        '200':
          description: successful operation
          content:
            application/text:
              schema:
                type: string
        '400':
          description: "Error"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /upload/generateWD+Metadata:
    post:
      tags:
        - Upload API
      summary: Generate wikified dataset and its metadata
      description: 'This method is the first part of the two-part detailed upload process. This method materializes the dataset resource given by the url,  performs entity resolution using Wikidata, augments the dataset with columns based on the entity resolution, and it returns the augmented dataset along with the metadata. <br>User can check and edit the results, and then submit them using the /upload/uploadWD+Metadata endpoint. <br>This method returns two list, first is a list of the materialized csv results, the second is a list of json file which indicates the WD+ metadata.<br><br> Two parameters are required: <br>**url**: The link to the target file. <br> **file_type**: A string indicate which type of the uploaded object is, currently only online_csv is available. '
      parameters:
        - name: url
          in: query
          schema:
            type: string
          required: true
          description: Url of input file
          example: 'https://raw.githubusercontent.com/usc-isi-i2/datamart-upload/rest_api_stable/datamart_web/test_upload_data.csv'
        - name: file_type
          in: query
          schema:
            type: string
            enum:
              - online_csv
          required: true
          description: Online file type, currently only support online_csv
      responses:
        '200':
          description: successful operation
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ProcessResult'
        '400':
          description: "Error"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /upload/uploadWD+Metadata:
    post:
      tags:
        - Upload API
      summary: upload data and metadata
      description: 'Second part of the two-part detailed upload process, the function uses the dataset and metadata returned from "/generateWD+Metadata" method and uploads them to datamart database.<br><br> Two parameters are required: <br>**data_input**: A list of items from materialized csv results from step 1. <br> **file_type**: A list of items from the WD+ metadata from step 1.'
      requestBody:
        content:
          multipart/form-data:
            schema:
              type: object
              properties:
                data_input:
                  type: string
                  description: 'Data(in list format) from /upload/generateWD+Metadata api, you can try to copy the content from <a href="https://raw.githubusercontent.com/usc-isi-i2/datamart-upload/rest_api_stable/datamart_web/sample_upload_part1_data.txt">here</a>'
                metadata:
                  type: string
                  description: 'Metadata(in list format) from /upload/generateWD+Metadata api, you can try to copy the content from <a href="https://raw.githubusercontent.com/usc-isi-i2/datamart-upload/rest_api_stable/datamart_web/sample_upload_part1_metadata.txt">here</a>'
              required:
                - data_input
                - metadata
      responses:
        '200':
          description: successful operation
          content:
            application/text:
              schema:
                type: string
        '400':
          description: "Error"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /embeddings/fb/{qnode}:
    get:
      tags:
        - Embeddings API
      summary: Fetch the FB embeddings for QNODE(s)
      description: '<p>The method fetches the FB embeddings for a given qnode(s)</p>'
      parameters:
        - name: qnode
          in: path
          schema:
            type: array
            items:
              type: string
          style: simple
          explode: false
          example: [Q738200, Q717195]
          required: true
          description: "<p>a list of Wikidata QNodes</p>"
      responses:
        '200':
          description: A csv file
          content:
            application/zip:
              schema:
                 type: string
                 format: binary
        '400':
          description: "Error"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

  /fuzzy_search/search:
    post:
      tags:
        - Search API
      summary: Search by keywords and geospatial_names, used for fuzzy search page
      description: |
        Find datasets by keywords and/or geospatial_names.

        **Attention:** The search results found here cannot be used for augmentation. If you want to use the search
          result for augmentation, please use **/search** endpoint instead!

        Required request body:
          * **query_json**: A JSON query to specify additional search constraints You can follow the example <a href="https://raw.githubusercontent.com/usc-isi-i2/datamart-userend/d3m/examples/sample_time_search_variables.json">here</a>. <br> Please see detail explainations on example part.
      requestBody:
        content:
          application/json:
            description: "keywords and geospatial information for search"
            schema:
              $ref: '#/components/schemas/Query'
            examples:
              keywords_searching:
                description: "Search using the keywords `rain`, with geospatial information at `Abay Chomen`, `Haru` and `Jarso`"
                value:
                  keywords:
                    - "rain"
                  geospatial_names:
                    - "Abay Chomen"
                    - "Haru"
                    - "Jarso"

      responses:
        '200':
          description: successful operation
          content:
            application/json:
              schema:
                type: array
                items:
                  $ref: '#/components/schemas/Results'
        '400':
          description: "Error"
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Error"

components:
  schemas:
    Query:
      $ref: "https://raw.githubusercontent.com/usc-isi-i2/datamart-upload/rest_api_stable/datamart_web/query_input_schema.json"
    Error:
      description: "JSON object returned by API on client errors"
      properties:
        error:
          type: string
          description: "The error message"
    Results:
      description: "Array of results returned by /search"
      type: array
      items:
        $ref: "#/components/schemas/Result"
    Result:
      $ref: "https://raw.githubusercontent.com/usc-isi-i2/datamart-upload/rest_api_stable/datamart_web/query_input_schema.json"
    # # # SearchResult:
    # #   type: object
    # #   properties:
    # #     summary:
    # #       type: object
    # #       properties:
    # #         title:
    # #           type: string
    # #         Datamart ID:
    # #           type: string
    # #         Score:
    # #           type: number
    # #         Url:
    # #           type: string
    # #         Columns:
    # #           type: array
    # #           items:
    # #             type: string
    # #         Recommend Join Columns:
    # #           type: string
    # #     datamart_id:
    # #       type: string
    # #     score:
    # #       type: number
    # #     materialize_info:
    # #       type: object
    # #       properties:
    # #         id:
    # #           type: string
    # #         score:
    # #           type: number
    # #         metadata:
    # #           type: object
    # #           properties:
    # #             connection_url:
    # #               type: string
    # #             search_result:
    # #               type: object
    # #               properties:
    # #                 p_nodes_needed:
    # #                   type: array
    # #                   items:
    # #                     type: string
    # #                 target_q_node_column_name:
    # #                   type: string
    # #             query_json:
    # #               type: string
    # #             search_type:
    # #               type: string
    # #         augmentation:
    # #           type: object
    # #           properties:
    # #             properties:
    # #               type: string
    # #             left_columns:
    # #               type: array
    # #               items:
    # #                 type: integer
    # #             right_columns:
    # #               type: array
    # #               items:
    # #                 type: integer
    # #             datamart_type:
    # #               type: string
    # #     metadata:
    # #       $ref: '#/components/schemas/Metadata'
    # # Metadata:
    # #   type: array
    # #   items:
    # #     type: object
    # #     properties:
    # #       selector:
    # #         type: array
    # #         items:
    # #           type: string
    # #       metadata:
    # #         type: object
    # #         properties:
    # #           structural_type:
    # #             type: string
    # #           semantic_types:
    # #             type: array
    # #             items:
    # #               type: string
    # #           dimension:
    # #             type: object
    # #             properties:
    # #               name:
    # #                 type: string
    # #               semantic_types:
    # #                 type: array
    # #                 items:
    # #                   type: string
    # #               length:
    # #                 type: integer
    # #           name:
    # #             type: string
    # ProcessResult:
    #   type: object
    #   properties:
    #     data:
    #       type: array
    #       items:
    #         type: string
    #     metadata:
    #       type: array
    #       items:
    #         type: object
    #         properties:
    #           datamart_id:
    #             type: string
    #           materialization:
    #             type: object
    #             properties:
    #               python_path:
    #                 type: string
    #               arguments:
    #                 type: object
    #                 properties:
    #                   url:
    #                     type: string
    #                   file_type:
    #                     type: string
    #           variables:
    #             type: array
    #             items:
    #               type: object
    #               properties:
    #                 datamart_id:
    #                   type: string
    #                 semantic_type:
    #                   type: array
    #                   items:
    #                     type: string
    #                 name:
    #                   type: string
    #                 description:
    #                   type: string
    #                 named_entity:
    #                   type: array
    #                   items:
    #                     type: string
    #                 temporal_coverage:
    #                   type: object
    #                   properties:
    #                     start:
    #                       type: string
    #                     end:
    #                       type: string
    #           title:
    #             type: string
    #           description:
    #             type: string
    #           keywords:
    #             type: array
    #             items:
    #               type: string
    #           url:
    #             type: string
    #           file_type:
    #             type: string
    #           xpath:
    #             type: string
    # TemporalVariable:
    #   type: object
    #   description: "Describes columns containing temporal information. DateTime format could be: '2019-01' or '2019-01-01' or '2019-01-01T10:10:10', and granularity should be 'month', 'day', 'second'"
    #   properties:
    #     start:
    #       type: string
    #       description: "Requested dates are more recent than this date."
    #     end:
    #       type: string
    #       description: "Requested dates are older than this date."
    #     granularity:
    #       type: string
    #       description: "Requested dates should match the requested granularity. For example, if 'day' is requested, the best match is a dataset with dates; however a dataset with hours is relevant too as hourly data can be aggregated into days."
    #       enum: [year, month,day,hour,minute,second]
tags:
  - name: Search API
  - name: Download API
  - name: Augment API
  - name: Upload API
  - name: Embeddings API
